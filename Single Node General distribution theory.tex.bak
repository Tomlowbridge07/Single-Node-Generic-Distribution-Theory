\documentclass[a4paper,10pt]{article}

\usepackage{color}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{comment}
\usepackage{natbib}
%\usepackage{float}
\usepackage{appendix}
\usepackage{subfig}
\usepackage{enumitem}
\usepackage{newfloat}
\usepackage[utf8]{inputenc}
\usepackage{floatrow}
\usepackage{bm}

\usetikzlibrary{calc}
\usetikzlibrary{fit}
\usetikzlibrary{shapes.misc,calc, positioning, hobby, backgrounds}


%\DeclarePairedDelimiter{\floor}{\lfloor}{\rightfloor}
%\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newcommand{\halflength}{\ensuremath{\floor{\frac{m}{2}}}}
\newcommand{\floor}[1]{\left \lfloor #1 \right \rfloor}
\newcommand{\ceil}[1]{\left \lceil #1 \right \rceil}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
 
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\theoremstyle{definition}
\newtheorem*{note}{Note}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareFloatingEnvironment[fileext=los,
    listname={List of Example Figures},
    name=Example Figure,
    placement=tbhp,
    within=section,]{examplefigure}

\title{Single Node general distribution theory}
\date{\today}
\author{Thomas Lowbridge \\ School of Mathematical Sciences \\ University of Nottingham}

\bibliographystyle{plain}

\begin{document}

\pagestyle{empty}
{
  \renewcommand{\thispagestyle}[1]{}
  \maketitle
  \tableofcontents  
}
\clearpage
\pagestyle{plain}


\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\newpage
\pagenumbering{arabic}
\section{Introduction to Problem}
We have some attack time distribution at a single node, $X$, with support, $\Pi=[0,\pi]$, and $B=\ceil{\pi}$.We also have the room cap size, $b$, we also have a cost $c$ and arrival rate, $\lambda$.

This gives us the state space, $(s,v)$ for $0 \leq s \leq B+1$ and $0 \leq v \leq b$.

We will consider the Cost to Progress matrix, $\mathbf{C}=\mathbf{A}+\mathbf{O}$, where $\mathbf{A}$ is the cost to not act due to arrival process and $\mathbf{O}$ is the cost to not act due to the observed process.

\textbf{Some Basic Properties}
Because the arrival process is not dependent on the observed number, $A_{i,j}=A_{i,1}$, that is $\mathbf{A}$ can be represented by a vector $\mathbf{a}=a_{i}=A_{i,1}$.

If $b=0$,we have $\mathbf{O}=\mathbf{0}$ , then $\mathbf{C}=\mathbf{A}$.

Because the observed process is dependent on $s$ and $v$ just multiples the answer by $v$. We can summarize the matrix $\mathbf{O}$ by $\mathbf{o}=O_{i,2}$. We can also note this is only possible if $b \geq 1$.

Then we can redefine, $C_{i,j}=a_{i}+jo_{i}$.



By our current definition $a_{i}= c \lambda \int_{i-1}^{i} F_{X}(x) dx$ and $o_{i}= c (F_{X}(i)-F_{X}(i-1))$.  

So $a_{i}$ is increasing.

If we want to decide when $C_{i,j}$ is increasing (non-decreasing) in $i,j$, by definition it is non-decreasing in $j$. so we will only look at it from the point of non-decreasing in $i$.

If $o_{i}$ is non-decreasing then clearly it is non-decreasing.

However if $o_{i}$ is decreasing then we must look closer. If once $o_{i}$ becomes decreasing it has all $c_{i,j} \geq c \lambda$, then we can avoid the problem.

We could also restrict the problem to some $b=b^{*}$. For each decreasing row, we can restrict the column to only allow $c_{i,j} \geq c \lambda$.

We can define $b_{i}^{+}=\floor{\frac{a_{i}-c \lambda}{o_{i}}}$ and a set $\chi=\{i \, | \, o_{i}<0 \}$ and define our limit by $b^{*}=\min\limits_{i \in \chi} \{b_{i}^{+} \}$

Let us consider the difference in $c_{i,j}$'s , $c_{i,j}-c_{i-1,j}=a_{i}-a_{i}+j(o_{i}-o_{i-1})$

We can define $c_{i,j}=c \int_{i-1}^{i} \lambda F_{X}(x)+j f_{x}(x) dx$. We can say this is non-decreasing if $\lambda F_{X}(x)+ j f_{x}(x)$ is non-decreasing, i.e $\lambda f_{X}(x) + j f^{'}_{{X}}(x) \geq 0$.


We can equivalently solve $\frac{\lambda}{j}f_{X}(x) \geq |f_{X}'(x)|$ for a function which is not strictly increasing. This is done by Gronwell's inequality theorem to get use $f(x_{0})e^{-\frac{\lambda}{j}|x-x_{0}|} \leq f(x) \leq f(x_{0})e^{\frac{\lambda}{j}|x-x_{0}|}$ for some $x_{0}$ in the support of $f_{X}$. This is equivalent to say that the function is not increasing/decreasing exponentially.

If we only consider decreasing regions of $f_{X}$ can we just check those, well lets say that in the regions $[a_{i},b_{i}]$ for $i \in {1,...,n}$ for some $n$. The function is decreasing then we could apply the same idea of Gronwell's inequality and get $f(p)e^{-\frac{\lambda}{j}|x-p|} \leq f(x) \leq f(p)e^{\frac{\lambda}{j}|x-p|}$ for some chosen $p \in [a_{i},b_{i}]$ (all choices are valid, easy choice is $p=a_{i}$) for all $i \in n$.

Then this allows us to say that in the decreasing regions of $f_{X}$ the equation still holds, and in the other regions it is increasing so the equation holds. Hence it holds.

So we really only need the decreasing regions to be monitored for exponential decay, if they decrease too fast we can be in trouble.

\section{Alternative explanation of their index}
We will first provide an alternative way to arrive at the index provided by the paper. Consider looking at the states. $s=1,2,...,B+1$, we can consider waiting $k$ time periods costing us $c_{s-1}+c_{s}+...+c_{s+k}$, then renewing, where $c_{i}=c \lambda \int_{i}^{i+1} F_{X}(t) dt$ is an increasing function. Comparing all strategies and then renewing to get $g$, the equilibrium value, for the amount of time over which we are comparing (which is up till the maximum).

Therefore from $s$ we will renew in $k$ if $g \leq c_{s+k}$, so renew now if $g \leq c_{s}$. That means to get the index for the state $s$, we need to work out $g(w)=\frac{w+c_{0}+...+c_{s-1}}{s}$, moving this into our bound gives

$$w \leq s c_{s} - \sum\limits_{i=0}^{s-1} c_{i}$$

which with the integrations is exactly the index found before.

\section{Introducing some observable parameter}
Let us consider having the previous set up but when the state is renewed a second parameter $v$ affects the cost function. That is to say each transnational cost has a second parameter, we will call the cost to transition from state $(s,v)$ to state $(s+1,v)$ is $c_{s,v}$. Again we are just comparing waiting $k$ time periods from the given state $(s,v)$. And again if $c_{s,v}$ is increasing in $s$ then we can get some results.


Now we are again comparing the cost of $c_{s-1,v}+...+c_{s+k,v}$ for $k=0,...,B-s+1$. Due to the functions increasing nature in time, we can  get that we renew now if $g \leq c_{s,v}$. Now to work out $g(\omega)$ since this depends on other $v$ we must find there plan, to renew.

We will define $\hat{\bm{s}}$ where $\hat{s}_{i}$ is the time to renew after in column $i$.

Therefore $\hat{s}_{i}=\max \{s' | c_{s',i} \leq c_{s,v}  \}$ and we note that $\hat{s}_{v}=s$.

With a plan of $\hat{\bm{s}}$ we can work out the long run average cost $g_{\hat{s}}(\omega)=\frac{\omega+\alpha}{\beta}$. Where $\alpha= \sum\limits_{i=v_{\text{min}}}^{v_{\text{max}}} (\sum\limits_{j=0}^{\hat{s}_{i}-1} c_{j,i}) P(V=i)$ is the expected cost due to waiting per renewal and $\beta=\sum\limits_{i=v_{\text{min}}}^{v_{\text{max}}} \hat{s}_{i} P(V=i) $ is the expected.

Therefore the index is

$$W(s,v)=\beta c_{s,v} - \alpha$$.

We note that this collapses to their index when $V \equiv v$ as this removes the dimensional component. We can also note that as nothing is requited of $v$, it is possible that $V: \Omega \rightarrow \mathbb{R}^{p}$ is a $p$ dimensional distribution. It is inconsequential in developing the index as long as the cost in time is increasing.

\section{Locally-Observable random attackers}
  
Consider our set up of local random attackers, we cannot easily say that our cost is increasing in time, due to the observable cost depending on time, we can however form it to be increasing by the use of the techniques discussed such as using a small enough $b$ (Using a $b \leq b_{crit}$) or checking our distribution satisfies certain conditions (Using $f(p)e^{-\frac{\lambda}{j}|x-p|} \leq f(x) \leq f(p)e^{\frac{\lambda}{j}|x-p|}$ in its decreasing regions).  
    
  
\section{Continuous version of the old set up}

We can form a continuous version of the problem. Let us consider a problem where the time since last visit $s$ state is now continuous and it only takes $r$ time to get to the node. Similarly to the theory we can limit $r \leq s \leq X_{max}+r$ (or $S_{max}$ for more generality).

We will have a cost density, $c(t)$ and cumulative cost to renew at time $s$, $C(s)= \int_{0}^{s+r} c(u) du$, which then renews the system. Again from a state $s$ we can choose to wait $k$ time before renewing. Assuming the cost density is increasing (like before), we will renew now if $g \leq \lim\limits_{i \rightarrow \infty} \frac{\int_{s+r}^{s+r+i} c(u) du}{i}$, now $g(\omega)=\frac{\omega + C(s)}{s+r}$  

So our index is from $\omega \leq (s+r) \lim\limits_{i \rightarrow \infty} \frac{\int_{s}^{s+i} c(u) du}{i} -C(s)$.

\textbf{Example 1.}
For a linearly increasing cost density, $c(t)=t$, we can work out that to renew at time $s$ we need $g \leq s+r$ and so$W=(s+r)^2 - \frac{(s+r)^2}{2}=\frac{(s+r)^2}{2}$.


Scaling our cost density, would allow us to think of this as the pdf. E.g. using $c(t)=\frac{2 x}{2.7^2}$ would be a pdf.

\textbf{Example 2.}
For uniform cost density (a non-decreasing density), we can work out that to renew at $s$, we need $g \leq 1$ and so $W(s)=s+r-(s+r)=0$


\section{Continuous (s,v) set up}
In our observable parameter set up, for each continuous $v$ at time $s$ we have a cost density $c(s,v)$ and similarly a cumulatively cost $C(s,v)=\int_{0}^{s+r} c(u,v) du$, again where $r$ is the return speed to that single node.

Again in state $(s,v)$, with an increasing cost density, we can wait $k$ time, we will renew now if $g \leq \lim\limits_{k \rightarrow \infty} \frac{\int_{s+r}^{s+r+k} c(u,v) du}{k} \equiv g_{s,v}$.

Again to work out $g(\omega)$ we need to know the full strategy at this particular limit $\hat{S}(i)= \begin{cases}
s \text{ when } i=v \\
\argmax\limits_{t \in [0,S_{max}]} \{g_{t,i} | g_{t,i} \leq g_{s,v} \} \text{ when } i \neq v
\end{cases}$.

We may wish to enforce continuity of this renewal function, however i am not sure doing this will gain anything. We will also note that we may redefine this function as

$$\hat{S}(i)=\arg\limits_{t} \left\{\lim\limits_{k \rightarrow 0^{+}} \frac{\int_{s+r}^{s+r+k} c(u,v) du - \int_{t+r}^{t+r+k} c(u,i) du}{k} =0  \right\}$$

due to the solving of the second part of the definition, we still need to assume the given $s$ value for $i=v$.

This gives a function which determines a renewal plan and therefore allows us to calculate $g(\omega)=\frac{\omega + \alpha}{\beta}$ where $\alpha= \int_{v_{min}}^{v_{max}} f_{v}(x)C(\hat{S}(x),x) dx$ and $\beta=\int_{v_{min}}^{v_{max}} f_{v}(x)\hat{S}(x) dx$.

Then again we will end up with the bound that $\omega \leq \beta g_{s,v} - \alpha$.


\textbf{Example 1.}
Consider a uniform cost density of $1$, then we expect the index to be the same at all points (as they look identical). Well due to this $\hat{S}(i) = s \forall i$. Therefore $g(\omega)=\frac{\omega + s}{s} \leq 1$ so $w \leq 0$. This index is constant and makes sense, as renewing gives us no benefit.


\textbf{Example 2.}
Consider the cost density as the pdf of some distribution, so our set up aligns with the patrol game. Let us say $X \sim U[0,2.7]$ and let us assume $V \sim U[1,2]$ with our costs scaled by this (by a scaling of increased attack rate $\lambda$), then $\lambda v$ is our cost multiplier and $r=1$, so our nodes take one time unit to visit. Then the cost density is the cdf of the distribution times lambda rate, so $c(s,v)= \lambda v F_{x}(s)$. overall cost So to determine $\hat{S}$, we just need solve the equation

$\lim\limits_{k \rightarrow 0^{+}} \frac{\int_{s+r}^{s+r+k} \lambda v \frac{u}{2.7} \ du - \int_{t+r}^{t+r+k} \lambda i \frac{u}{2.7} du}{k} =0 \iff t=\frac{v}{i} (s+r) -r$
 
We note that L`hoptials rule may help to calculate such limits. 

so $\hat{S}(i)=\frac{v}{i} (s+r) -r$

and so our index is $W(s,v)=\frac{\lambda v (s+r) (\ln (2) v (s+r) -2r)}{5.4}$.

Can we consider a `end state' as we were using before, as when consider $\hat{S}$ for $(2.7,2)$ when $r=0$ we are at the end of this and as the cost density is only increasing in $v$ we expect $\hat{S}$ to be more than 2.7 for every other $i \in V$. Does this cause a problem, one would think not as the cost density is still valid past the value of $2.7$.

\section{Particular examples of (s,v) adaptations}
\begin{itemize}
\item $v$ affects the arrival rate for the next stream
\item $v$ affects the cost, $c$ (Really the same as above, cost and arrival rate increases can be seen as one another in the expectation)
\item $v$ denotes the number of `local observations'
\end{itemize}

\section{Movement in the parametrized dimension}
We may wish to consider a process which when left to it, may drift into different parameter states. Let us consider the discrete time epoch theory, we can consider that in a state $(s,v)$, there is a transition probability to $(s+1,i)$ for $i \in V$ with some probability $p_{(s,v) \rightarrow (s+1,i)}$.

It is intuitive that if this probability only takes us into worse $i \in V$ then we have no real issue with our theory. That is if we have an increasing cost density in $v$ as well as in $s$ and parametrized states can only increase, then we have no issue, as the cost can only get worse. This allows us to follow the pattern of considering the immediate future for $g$'s bound.

The question is what is $g$'s bound now, I believe intuitively it will be the expected immediate cost, as will consider the action of wait and progress costing an amount multiplied by the transitional probability. We may think there is an issue as the end location may not follow the pattern of renewing immediately, however limiting ourselves to transitions only going to `worse' (states with a higher $g_{s,v}$) we can guarantee our pattern still holds.

However if transitions can move us to better states this is not necessarily true. What do we do in this situation, well it is manually possible to calculate all the waiting decisions (under expectation of future) and pick the best.

Assuming our cost density is still increasing in $s$ and $v$ then moving to lower $i < v$ does not get us any solid property.

However if we can say all future waiting decisions (expect wait 0) are (under expectation) worse than wait 0 ,then we can still state our theory as before and use our $g \leq g_{s,v}$ idea. So what we need for this to be true, We need the expected future cost to be, on average, worse than the initial cost.

\section{Movement in the only worsening parameter direction}

Let us now assume, upon the choose to not visit the system in state $(s,v)$ that it transitions to the state $(s+1,i)$ with probability $p_{(s,v) \rightarrow (s+1,i)}$, with $p_{(s,v) \rightarrow (s+1,i)}=0$ for $i < v$.

Then we can again use our theory, now $g \leq g_{s,v}$ exactly as before and we can still get the same $\hat{S}$ as before, now what will change is the question ?

Well the $g(\omega)$ will need to be worked out by paths and not simply by looking at the lines. We still need to work out the expected cost and the expected length per renewal, however this is a little more complicated, but not unachievable.

To do so it will be better to convert out $\hat{S}$ into a $\hat{V}$ rule in which we are dealing with rows, renewing if $v \geq \hat{v}_{i}$. We can only perform such a conservation if $\hat{S}$ is decreasing, we can do it without this but it is more notationally heavy, as each element is a set of $v$ we will renew at.

To make the conversion we will use the rule that $\hat{v}_{i}= \left| \{ \hat{s}_{j} \, | \, \hat{s}_{j} \leq i , \hat{s}_{j} \in \hat{S} \}\right|+1$.

Using this it possible to write the new expected cost, $\alpha=\sum\limits_{V} P(V=v) (C_{1,v}+ \sum\limits_{j=2}^{B+1} \sum\limits_{i \leq \hat{v}_{j}} P_{1,v}(j,i) C_{j,i})$, and expected length $\beta=\sum\limits_{V} P(V=v) (1+ \sum\limits_{j=2}^{B+1} \sum\limits_{i \leq \hat{v}_{j}} P_{1,v}(j,i))$. Where $P_{1,v}(j,i)$ is the probability of being in state $(j,i)$ after $j-1$ transitions.

\textbf{Example 1.}
If we use $p_{(s,v) \rightarrow (s+1,v+1)}=1$, except when $v=v_{max}$ in which case $p_{(s,v_{max}) \rightarrow (s+1,v_{max})}=1$, then all the transitional movements are deterministic and $P_{1,v}(j,i)=1 \iff i=v+j-1$ and $0$ otherwise. This means our $\alpha=\sum\limits_{V} P(V=v) (C_{1,v}+ \sum\limits_{i=2}^{B+1} \mathbb{I}(v+i-1 < \hat{v}_{j})C_{i,v+i-1)$ and $\beta= \sum\limits_{V} P(V=v) (1+ \sum\limits_{i=2}^{B+1} \mathbb{I}(v+i-1 < \hat{v}_{j}))$


\textbf{Example 2.}
Consider for some $p \in [0,1]$ $p_{(s,v) \rightarrow (s+1,v)}=1-p$ and $p_{(s,v) \rightarrow (s+1,v+1)}=p$
%End of main part of document
\bibliography{mybib}

\newpage
\appendix
\pagenumbering{roman}
\appendixpage
\addappheadtotoc
\section{First appendix section}

\end{document}
